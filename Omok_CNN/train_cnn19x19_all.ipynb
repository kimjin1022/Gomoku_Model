{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae691ef-3744-4fe3-9a66-ec631d811bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 02:30:03.050025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 02:30:03.144769: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699134c3-a5b5-4e88-a12d-8e89805b1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 02:30:14.475835: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 02:30:16.111666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14825 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:3b:00.0, compute capability: 7.5\n",
      "2024-06-01 02:30:16.112763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14825 MB memory:  -> device: 1, name: Quadro RTX 5000, pci bus id: 0000:5e:00.0, compute capability: 7.5\n",
      "2024-06-01 02:30:16.113764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14825 MB memory:  -> device: 2, name: Quadro RTX 5000, pci bus id: 0000:86:00.0, compute capability: 7.5\n",
      "2024-06-01 02:30:16.114710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14825 MB memory:  -> device: 3, name: Quadro RTX 5000, pci bus id: 0000:af:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Running on multiple GPUs  ['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3']\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
    "print('\\n\\n Running on multiple GPUs ', [gpu.name for gpu in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645e6e85-c949-4e05-a13a-de5c0e6df06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 24938/24938 [00:53<00:00, 468.38it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     x_data\u001b[38;5;241m.\u001b[39mextend(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m     y_data\u001b[38;5;241m.\u001b[39mextend(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 13\u001b[0m x_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(x_data, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat4\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, h, w, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m y_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_data, np\u001b[38;5;241m.\u001b[39mfloat4)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, h \u001b[38;5;241m*\u001b[39m w))\n\u001b[1;32m     16\u001b[0m x_train, x_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(x_data, y_data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2020\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/tf-gpu/lib/python3.8/site-packages/numpy/__init__.py:320\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float4'"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    w, h = 19, 19\n",
    "    base_path = os.path.join('dataset3', '*/*.npz')\n",
    "    \n",
    "    file_list = glob(base_path)\n",
    "    \n",
    "    x_data, y_data = [], []\n",
    "    for file_path in tqdm(file_list):\n",
    "        data = np.load(file_path)\n",
    "        x_data.extend(data['inputs'])\n",
    "        y_data.extend(data['outputs'])\n",
    "    \n",
    "    x_data = np.array(x_data, np.float4).reshape((-1, h, w, 1))\n",
    "    y_data = np.array(y_data, np.float4).reshape((-1, h * w))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=2020)\n",
    "    \n",
    "    del x_data, y_data\n",
    "    \n",
    "    print(x_train.shape, y_train.shape)\n",
    "    print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dc170a-f234-42f1-bead-bfb7468978b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 19, 19, 32)        1600      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 19, 19, 64)        100416    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 19, 19, 128)       401536    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 19, 19, 64)        401472    \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 19, 19, 32)        100384    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 19, 19, 1)         33        \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 361)               0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 361)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,005,441\n",
      "Trainable params: 1,005,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, 7, activation='relu', padding='same', input_shape=(19, 19, 1)),\n",
    "        layers.Conv2D(64, 7, activation='relu', padding='same',),\n",
    "        layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
    "#        layers.Conv2D(256, 7, activation='relu', padding='same'),\n",
    "#        layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
    "        layers.Conv2D(64, 7, activation='relu', padding='same'),\n",
    "        layers.Conv2D(32, 7, activation='relu', padding='same'),\n",
    "        layers.Conv2D(1, 1, activation=None, padding='same'),  \n",
    "        layers.Reshape((h * w,)),\n",
    "        layers.Activation('sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a050af-6fb3-4eb3-aab8-e9345174e8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 02:25:59.449718: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 12.85GiB (rounded to 13800683520)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-06-01 02:25:59.449782: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2024-06-01 02:25:59.449808: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 21, Chunks in use: 21. 5.2KiB allocated for chunks. 5.2KiB in use in bin. 968B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449824: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 1, Chunks in use: 1. 512B allocated for chunks. 512B in use in bin. 512B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449840: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449854: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449868: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 6.2KiB allocated for chunks. 6.2KiB in use in bin. 6.1KiB client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449882: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 8.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449896: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449908: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449921: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449934: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449952: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 2, Chunks in use: 2. 784.0KiB allocated for chunks. 784.0KiB in use in bin. 784.0KiB client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449966: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 784.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449981: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 2, Chunks in use: 2. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.\n",
      "2024-06-01 02:25:59.449995: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 0. 2.68MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.450008: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.450021: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.450034: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.450047: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.450059: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.450072: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-06-01 02:25:59.450089: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 1. 14.47GiB allocated for chunks. 12.85GiB in use in bin. 12.85GiB client-requested in use in bin.\n",
      "2024-06-01 02:25:59.450105: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 12.85GiB was 256.00MiB, Chunk State: \n",
      "2024-06-01 02:25:59.450132: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 1.62GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 12.85GiB | Requested Size: 12.85GiB | in_use: 1 | bin_num: -1\n",
      "2024-06-01 02:25:59.450144: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 15545466880\n",
      "2024-06-01 02:25:59.450157: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000000 of size 256 next 1\n",
      "2024-06-01 02:25:59.450169: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000100 of size 1280 next 2\n",
      "2024-06-01 02:25:59.450180: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000600 of size 256 next 3\n",
      "2024-06-01 02:25:59.450190: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000700 of size 256 next 4\n",
      "2024-06-01 02:25:59.450200: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000800 of size 256 next 5\n",
      "2024-06-01 02:25:59.450210: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000900 of size 256 next 7\n",
      "2024-06-01 02:25:59.450220: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000a00 of size 256 next 8\n",
      "2024-06-01 02:25:59.450230: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000b00 of size 256 next 6\n",
      "2024-06-01 02:25:59.450239: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000c00 of size 256 next 9\n",
      "2024-06-01 02:25:59.450249: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000d00 of size 256 next 14\n",
      "2024-06-01 02:25:59.450259: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000e00 of size 256 next 12\n",
      "2024-06-01 02:25:59.450269: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04000f00 of size 256 next 13\n",
      "2024-06-01 02:25:59.450280: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001000 of size 512 next 19\n",
      "2024-06-01 02:25:59.450291: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001200 of size 256 next 22\n",
      "2024-06-01 02:25:59.450301: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001300 of size 256 next 24\n",
      "2024-06-01 02:25:59.450311: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001400 of size 256 next 17\n",
      "2024-06-01 02:25:59.450320: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001500 of size 256 next 18\n",
      "2024-06-01 02:25:59.450331: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001600 of size 256 next 27\n",
      "2024-06-01 02:25:59.450340: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001700 of size 256 next 26\n",
      "2024-06-01 02:25:59.450350: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001800 of size 256 next 28\n",
      "2024-06-01 02:25:59.450360: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001900 of size 256 next 29\n",
      "2024-06-01 02:25:59.450370: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001a00 of size 256 next 30\n",
      "2024-06-01 02:25:59.450380: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04001b00 of size 256 next 31\n",
      "2024-06-01 02:25:59.450390: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f6d04001c00 of size 8704 next 10\n",
      "2024-06-01 02:25:59.450400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d04003e00 of size 6400 next 11\n",
      "2024-06-01 02:25:59.450411: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f6d04005700 of size 802816 next 16\n",
      "2024-06-01 02:25:59.450422: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d040c9700 of size 401408 next 15\n",
      "2024-06-01 02:25:59.450432: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d0412b700 of size 401408 next 25\n",
      "2024-06-01 02:25:59.450442: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f6d0418d700 of size 2809856 next 21\n",
      "2024-06-01 02:25:59.450453: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d0443b700 of size 1605632 next 20\n",
      "2024-06-01 02:25:59.450463: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d045c3700 of size 1605632 next 23\n",
      "2024-06-01 02:25:59.450476: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f6d0474b700 of size 13800683520 next 32\n",
      "2024-06-01 02:25:59.450487: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f703b0a6f00 of size 1737134336 next 18446744073709551615\n",
      "2024-06-01 02:25:59.450497: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2024-06-01 02:25:59.450510: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 21 Chunks of size 256 totalling 5.2KiB\n",
      "2024-06-01 02:25:59.450521: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 512 totalling 512B\n",
      "2024-06-01 02:25:59.450533: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-06-01 02:25:59.450544: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6400 totalling 6.2KiB\n",
      "2024-06-01 02:25:59.450557: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 401408 totalling 784.0KiB\n",
      "2024-06-01 02:25:59.450569: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 1605632 totalling 3.06MiB\n",
      "2024-06-01 02:25:59.450581: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 13800683520 totalling 12.85GiB\n",
      "2024-06-01 02:25:59.450593: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 12.86GiB\n",
      "2024-06-01 02:25:59.450605: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 15545466880 memory_limit_: 15545466880 available bytes: 0 curr_region_allocation_bytes_: 31090933760\n",
      "2024-06-01 02:25:59.450623: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                     15545466880\n",
      "InUse:                     13804711168\n",
      "MaxInUse:                  13804711168\n",
      "NumAllocs:                          59\n",
      "MaxAllocSize:              13800683520\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-06-01 02:25:59.450641: W tensorflow/tsl/framework/bfc_allocator.cc:492] *****************************************************************************************___________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/tf-gpu/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/micromamba/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    start_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=256,\n",
    "        epochs=10,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint('./models/%s.h5' % (start_time), monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "            ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, mode='auto')\n",
    "        ],\n",
    "        validation_data=(x_val, y_val),\n",
    "        use_multiprocessing=True,\n",
    "        workers=16\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
