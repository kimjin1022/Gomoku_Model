{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae691ef-3744-4fe3-9a66-ec631d811bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 17:34:28.679551: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 17:34:28.790978: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699134c3-a5b5-4e88-a12d-8e89805b1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 17:34:48.629598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 17:34:50.270221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14825 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:3b:00.0, compute capability: 7.5\n",
      "2024-05-26 17:34:50.271330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14825 MB memory:  -> device: 1, name: Quadro RTX 5000, pci bus id: 0000:5e:00.0, compute capability: 7.5\n",
      "2024-05-26 17:34:50.272236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14825 MB memory:  -> device: 2, name: Quadro RTX 5000, pci bus id: 0000:86:00.0, compute capability: 7.5\n",
      "2024-05-26 17:34:50.273196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14825 MB memory:  -> device: 3, name: Quadro RTX 5000, pci bus id: 0000:af:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Running on multiple GPUs  ['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3']\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
    "print('\\n\\n Running on multiple GPUs ', [gpu.name for gpu in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645e6e85-c949-4e05-a13a-de5c0e6df06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 10797/10797 [00:36<00:00, 295.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4146931, 19, 19, 1) (4146931, 361)\n",
      "(1036733, 19, 19, 1) (1036733, 361)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    w, h = 19, 19\n",
    "    base_path = os.path.join('dataset', '*/*.npz')\n",
    "    \n",
    "    file_list = glob(base_path)\n",
    "    \n",
    "    x_data, y_data = [], []\n",
    "    for file_path in tqdm(file_list):\n",
    "        data = np.load(file_path)\n",
    "        x_data.extend(data['inputs'])\n",
    "        y_data.extend(data['outputs'])\n",
    "    \n",
    "    x_data = np.array(x_data, np.float32).reshape((-1, h, w, 1))\n",
    "    y_data = np.array(y_data, np.float32).reshape((-1, h * w))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=2020)\n",
    "    \n",
    "    del x_data, y_data\n",
    "    \n",
    "    print(x_train.shape, y_train.shape)\n",
    "    print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dc170a-f234-42f1-bead-bfb7468978b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 19, 19, 64)        3200      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 19, 19, 128)       401536    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 19, 19, 256)       1605888   \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 19, 19, 128)       1605760   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 19, 19, 64)        401472    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 19, 19, 1)         65        \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 361)               0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 361)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,017,921\n",
      "Trainable params: 4,017,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, 7, activation='relu', padding='same', input_shape=(19, 19, 1)),\n",
    "        layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
    "        layers.Conv2D(256, 7, activation='relu', padding='same'),\n",
    "        layers.Conv2D(128, 7, activation='relu', padding='same'),\n",
    "        layers.Conv2D(64, 7, activation='relu', padding='same'),\n",
    "        layers.Conv2D(1, 1, activation=None, padding='same'),\n",
    "        layers.Reshape((h * w,)),\n",
    "        layers.Activation('sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a050af-6fb3-4eb3-aab8-e9345174e8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 17:36:36.528557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2024-05-26 17:36:37.337009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2024-05-26 17:36:38.196152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2024-05-26 17:36:38.577247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2024-05-26 17:36:38.614526: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-26 17:36:38.617623: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:38.978603: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:39.397526: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:39.836543: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:42.262644: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7efc07ff3e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-26 17:36:42.262704: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-05-26 17:36:42.262722: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-05-26 17:36:42.262733: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-05-26 17:36:42.262744: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (3): Quadro RTX 5000, Compute Capability 7.5\n",
      "2024-05-26 17:36:42.269336: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-26 17:36:42.339396: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-26 17:36:42.339517: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:42.340332: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:42.345818: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:42.348140: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:42.396173: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-05-26 17:36:42.762706: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:42.767483: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2024-05-26 17:36:42.895380: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16199/16199 [==============================] - ETA: 0s - loss: 0.0071 - acc: 0.5339"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 17:46:09.662512: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 430.59MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-05-26 17:46:09.735158: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 845.17MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_acc improved from -inf to 0.59073, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 580s 35ms/step - loss: 0.0071 - acc: 0.5339 - val_loss: 0.0057 - val_acc: 0.5907 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "16199/16199 [==============================] - ETA: 0s - loss: 0.0055 - acc: 0.6041\n",
      "Epoch 2: val_acc improved from 0.59073 to 0.60709, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 562s 35ms/step - loss: 0.0055 - acc: 0.6041 - val_loss: 0.0055 - val_acc: 0.6071 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "16199/16199 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.6221\n",
      "Epoch 3: val_acc improved from 0.60709 to 0.61853, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 561s 35ms/step - loss: 0.0052 - acc: 0.6221 - val_loss: 0.0053 - val_acc: 0.6185 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "16198/16199 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.6368\n",
      "Epoch 4: val_acc improved from 0.61853 to 0.62567, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 562s 35ms/step - loss: 0.0050 - acc: 0.6368 - val_loss: 0.0052 - val_acc: 0.6257 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "16199/16199 [==============================] - ETA: 0s - loss: 0.0049 - acc: 0.6510\n",
      "Epoch 5: val_acc improved from 0.62567 to 0.63334, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 562s 35ms/step - loss: 0.0049 - acc: 0.6510 - val_loss: 0.0052 - val_acc: 0.6333 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "16198/16199 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.6646\n",
      "Epoch 6: val_acc improved from 0.63334 to 0.63739, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 562s 35ms/step - loss: 0.0047 - acc: 0.6646 - val_loss: 0.0051 - val_acc: 0.6374 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "16199/16199 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.6784\n",
      "Epoch 7: val_acc improved from 0.63739 to 0.64354, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 562s 35ms/step - loss: 0.0046 - acc: 0.6784 - val_loss: 0.0051 - val_acc: 0.6435 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "16199/16199 [==============================] - ETA: 0s - loss: 0.0044 - acc: 0.6918\n",
      "Epoch 8: val_acc improved from 0.64354 to 0.64716, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 563s 35ms/step - loss: 0.0044 - acc: 0.6918 - val_loss: 0.0051 - val_acc: 0.6472 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "16199/16199 [==============================] - ETA: 0s - loss: 0.0043 - acc: 0.7043\n",
      "Epoch 9: val_acc improved from 0.64716 to 0.65272, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 563s 35ms/step - loss: 0.0043 - acc: 0.7043 - val_loss: 0.0051 - val_acc: 0.6527 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "16199/16199 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.7163\n",
      "Epoch 10: val_acc improved from 0.65272 to 0.65794, saving model to ./models/20240526_173611.h5\n",
      "16199/16199 [==============================] - 563s 35ms/step - loss: 0.0041 - acc: 0.7163 - val_loss: 0.0051 - val_acc: 0.6579 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    start_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=256,\n",
    "        epochs=10,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint('./models/%s.h5' % (start_time), monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "            ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=1, mode='auto')\n",
    "        ],\n",
    "        validation_data=(x_val, y_val),\n",
    "        use_multiprocessing=True,\n",
    "        workers=16\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
